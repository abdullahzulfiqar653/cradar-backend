from textwrap import dedent

from django.db.models import QuerySet
from django.utils.translation import gettext
from langchain.prompts import ChatPromptTemplate
from langchain_openai.chat_models import ChatOpenAI
from rest_framework import exceptions

from api.ai import config
from api.ai.generators.utils import token_tracker
from api.models.asset import Asset
from api.models.takeaway import Takeaway
from api.models.user import User
from api.utils.lexical import LexicalProcessor


def construct_prompt(asset: Asset, instruction: str, takeaways: QuerySet[Takeaway]):
    prompt_format = gettext(
        dedent(
            """
                <instruction>{instruction}</instruction>

                <context>
                <content>
                {content}
                </content>

                <notes>
                {notes}
                </notes>
                </context>
            """
        )
    )
    notes = "- " + "\n- ".join([takeaway.title for takeaway in takeaways[:20]])
    content = LexicalProcessor(asset.content["root"]).to_markdown()
    prompt = prompt_format.format(instruction=instruction, content=content, notes=notes)
    return prompt


def generate_content(
    asset: Asset, instruction: str, takeaways: QuerySet[Takeaway], created_by: User
):
    if takeaways.count() < 2:
        raise exceptions.ValidationError("Not enough takeaways to analyze.")
    system_prompt = dedent(
        """
            The user is working on a analysis report. 
            Follow the user's instruction to generate the content for the report based on the given context,
            which includes the notes that user has made and the current report content.
            The tag <cursor/> indicates where the user wants the generated content to be placed.
            The generated content should be relevant to the context and should be in a readable format.
        """
    )
    human_prompt = construct_prompt(asset, instruction, takeaways)
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                gettext(system_prompt),
            ),
            ("human", "{human_prompt}"),
        ]
    )
    llm = ChatOpenAI(model=config.model)
    chain = prompt | llm
    with token_tracker(asset.project, asset, "generate-asset", created_by):
        output = chain.invoke({"human_prompt": human_prompt})
    return output.content
